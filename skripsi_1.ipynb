{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8Ybu3cIbLE4",
        "outputId": "2316d7c9-0fc1-4a2c-e599-ba548dcf88c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m523.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install Keras-Preprocessing\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, AveragePooling2D, Flatten\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install split-folders"
      ],
      "metadata": {
        "id": "CRXHu1jNfabU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import splitfolders\n",
        "# splitfolders.fixed(\n",
        "#     \"/content/drive/MyDrive/skripsi/train\",\n",
        "#     output=\"/content/drive/MyDrive/old_ratio\",\n",
        "#     seed=1337,\n",
        "#     fixed=(200, 50),\n",
        "#     oversample=False,\n",
        "#     group_prefix=None\n",
        "# )\n"
      ],
      "metadata": {
        "id": "aM05WEFnbWsq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/MyDrive/old_ratio/train'\n",
        "val_dir = '/content/drive/MyDrive/old_ratio/val'\n",
        "test_dir = '/content/drive/MyDrive/old_ratio/test'\n"
      ],
      "metadata": {
        "id": "ErZW04_IcrMD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_metric(y_true, y_pred):\n",
        "  def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0,1)))\n",
        "    recall = (true_positives + K.epsilon())/ (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "  def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0,1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = (true_positives + K.epsilon()) / ( predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "  precision_value = precision(y_true, y_pred)\n",
        "  recall_value = recall(y_true, y_pred)\n",
        "  f1_score = 2 * ((precision_value * recall_value) / (precision_value + recall_value + K.epsilon()))\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "HI89DRjfczK0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nilai_bs = 4\n",
        "nilai_lr = 0.0001\n",
        "IMG_SIZE = 224\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255.,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "train = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=nilai_bs,\n",
        ")\n",
        "validation = train_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    shuffle=False,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=nilai_bs,\n",
        ")\n"
      ],
      "metadata": {
        "id": "SWtDx78Lc1Ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffbca5b-22ef-49bf-dff0-1414aa49f96a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 762 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        ")\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    AveragePooling2D(pool_size=(1, 1)),\n",
        "    Flatten(),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "CFWAhUOyc9jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96f3c39-11d6-48be-ed80-8e67e275ac78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session()\n"
      ],
      "metadata": {
        "id": "si8_AafyhQmM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Optimizer = tf.keras.optimizers.Adam(learning_rate=nilai_lr)\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=Optimizer,\n",
        "    metrics=['accuracy', f1_metric]\n",
        ")\n",
        "nilai_epoch = 30\n",
        "nilai_patience = 8\n",
        "my_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_f1_metric',\n",
        "    patience=nilai_patience\n",
        ")\n",
        "history = model.fit(\n",
        "    train,\n",
        "    epochs=nilai_epoch,\n",
        "    batch_size=nilai_bs,\n",
        "    validation_data=validation,\n",
        "    callbacks=my_callback,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7dGHEcuadAuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd227a0-74ab-4eca-ca99-7e7cae26f005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "191/191 [==============================] - 704s 3s/step - loss: 0.7182 - accuracy: 0.7743 - f1_metric: 0.7749 - val_loss: 1.0631 - val_accuracy: 0.4800 - val_f1_metric: 0.4800\n",
            "Epoch 2/30\n",
            "191/191 [==============================] - 690s 4s/step - loss: 0.4988 - accuracy: 0.8793 - f1_metric: 0.8796 - val_loss: 3.1278 - val_accuracy: 0.5000 - val_f1_metric: 0.5000\n",
            "Epoch 3/30\n",
            "191/191 [==============================] - 634s 3s/step - loss: 0.4385 - accuracy: 0.8885 - f1_metric: 0.8887 - val_loss: 1.6920 - val_accuracy: 0.5400 - val_f1_metric: 0.5400\n",
            "Epoch 4/30\n",
            "191/191 [==============================] - 628s 3s/step - loss: 0.3743 - accuracy: 0.8911 - f1_metric: 0.8914 - val_loss: 0.4650 - val_accuracy: 0.8000 - val_f1_metric: 0.8000\n",
            "Epoch 5/30\n",
            "117/191 [=================>............] - ETA: 3:29 - loss: 0.2566 - accuracy: 0.9145 - f1_metric: 0.9145"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = min(history.history['loss'])\n",
        "best_accuracy = max(history.history['accuracy'])\n",
        "best_val_loss = min(history.history['val_loss'])\n",
        "best_val_accuracy = max(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "V23NUJUAL-Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HZHDkOauMEDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Training Accuracy: %.4f\" % best_accuracy)\n",
        "print(\"Best Loss: %.4f\" % best_loss)\n",
        "print(\"Best Validation Accuracy: %.4f\" % best_val_accuracy)\n",
        "print(\"Best Validation Loss: %.4f\" % best_val_loss)\n",
        "\n",
        "df_hist = pd.DataFrame(history.history)\n",
        "hist_csv_file = '/content/drive/MyDrive/skripsi/Skenario_30.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    df_hist.to_csv(f)\n",
        "model.save('/content/drive/MyDrive/skripsi/Skenario_30.h5')\n"
      ],
      "metadata": {
        "id": "rPg7v0ahdOeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(history, metrics):\n",
        "  for metric in metrics:\n",
        "    train_metrics = history.history[metric]\n",
        "    val_metrics = history.history['val_'+metric]\n",
        "    epochs = range(1, len(train_metrics) + 1)\n",
        "    plt.plot(epochs, train_metrics)\n",
        "    plt.plot(epochs, val_metrics)\n",
        "    plt.title(\"Training and Validation \" + metric)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([\"train_\"+metric, \"val_\"+metric])\n",
        "    plt.show()\n",
        "plot_metric(history, [\"accuracy\", \"loss\"])\n"
      ],
      "metadata": {
        "id": "k02XtIKAbiNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    shuffle=False,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=nilai_bs,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "Y_pred = model.predict(test_generator, workers=0)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y_true = test_generator.classes[test_generator.index_array]\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=test_generator.class_indices\n",
        ")\n",
        "cm_display.plot(cmap=\"OrRd\")\n",
        "plt.show()\n",
        "\n",
        "target_names = test_generator.class_indices\n",
        "print(\"Hasil Klasifikasi\")\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "eR-Ei8maksy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oUtBRyv-NRKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Fungsi f1_metric\n",
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = np.sum(np.round(np.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = np.sum(np.round(np.clip(y_true, 0, 1)))\n",
        "    predicted_positives = np.sum(np.round(np.clip(y_pred, 0, 1)))\n",
        "\n",
        "    precision = true_positives / (predicted_positives + 1e-7)\n",
        "    recall = true_positives / (possible_positives + 1e-7)\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
        "    return f1_score\n",
        "\n",
        "# Path ke model yang telah dilatih\n",
        "model_path = '/content/drive/MyDrive/skripsi/Skenario_30.h5'\n",
        "\n",
        "# Memuat model dengan menyertakan fungsi khusus (f1_metric)\n",
        "model = load_model(model_path, custom_objects={'f1_metric': f1_metric})\n",
        "\n",
        "# Path ke gambar yang ingin diuji\n",
        "image_path = \"/content/drive/MyDrive/skripsi/validation/no pedestrian/val (111).jpg\"  # Ganti dengan path gambar Anda\n",
        "\n",
        "# Membaca gambar menggunakan OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "img_height, img_width, _ = image.shape\n",
        "\n",
        "# Melakukan resizing gambar sesuai dengan kebutuhan model\n",
        "IMG_SIZE = 224\n",
        "image_resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "image_resized = np.expand_dims(image_resized, axis=0)  # Menambahkan dimensi batch\n",
        "image_resized = image_resized.astype('float32') / 255.0  # Normalisasi\n",
        "\n",
        "# Melakukan prediksi dengan model\n",
        "prediction = model.predict(image_resized)\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "if prediction[0][0] > prediction[0][1]:\n",
        "    print(\"Gambar tersebut adalah bukan pejalan kaki.\")\n",
        "else:\n",
        "    print(\"Gambar tersebut adalah pejalan kaki.\")\n"
      ],
      "metadata": {
        "id": "a8xyJ3QKOQsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}